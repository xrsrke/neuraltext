{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Dataset\n",
    "\n",
    "> Generating synthetic dataset for character, and sentence neural activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Tuple, Set\n",
    "import random\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchtyping import TensorType\n",
    "\n",
    "from neuraltext.dataset import get_vocabs\n",
    "from neuraltext.utils import mat2dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_snippet(character: str, path: str = \"./data/snippets/t5.2020.01.13_snippets.mat\"):\n",
    "    vocabs = get_vocabs().keys()\n",
    "    assert character in vocabs, f\"Vocab {character} not found\"\n",
    "\n",
    "    # TODO: don't hardcode >\n",
    "    character = \"greaterThan\" if character == \">\" else character\n",
    "    snippets = mat2dict(path)\n",
    "    return snippets[character]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sythetic Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def generate_random_sentence(\n",
    "    length: int, # The number of words in the sentence\n",
    "    vocab_path: str = \"./data/english/1000-english-words.json\" # The path to the vocabulary\n",
    ") -> str:\n",
    "    with open(vocab_path, \"r\") as f:\n",
    "        words = json.load(f)\n",
    "    n_words = len(words)\n",
    "    idxs = random.sample(range(n_words), length)\n",
    "    text = \" \".join([words[str(idx)] for idx in idxs])\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the explanation for the code above:\n",
    "1. We first load the snippet library that we created in Step 2.\n",
    "2.  We then create a list of words that we will use to generate our synthetic sentences.  We use the 'rare' word list\n",
    "    file to increase the frequency of words with rare letters ('z', 'x', 'j', 'q'). The rare word file contains the\n",
    "    indices of the words in 'wordListFile' with rare letters.\n",
    "3.  We then generate our synthetic neural data by calling the 'makeSyntheticDataFromRawSnippets' function.\n",
    "4.  We then combine the character probabilities with the character start signal.\n",
    "5.  We then cut off the first part of the data so the RNN starts off \"hot\" randomly in the middle of text.\n",
    "6.  We then bin the data if 'binSize' is greater than 1.\n",
    "7.  We then create an error mask that doesn't penalize the RNN for errors that occur before the first character starts.\n",
    "8.  We then save the synthetic data to a . "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\" Here is the explanation for the code above:\n",
    "1. First, we define the function named makeSyntheticDataFromRawSnippets\n",
    "2. Then we define the arguments it takes in.\n",
    "3. Next, we define the variables that we will use in the function.\n",
    "4. Then we start a for loop that will iterate over all of the sentences we want to generate.\n",
    "5. Then we define the variables that we will use inside of the for loop.\n",
    "6. Then we generate this sentence one character at a time.\n",
    "7. Then we pick a new word if needed.\n",
    "8. Then we pick the character snippet to use for the current character.\n",
    "9. Then we copy the snippet for the current character.\n",
    "10. Then we linearly time warp the current snippet to add more variability.\n",
    "11. Then we randomly add in 'blank' pauses with some probability.\n",
    "12. Then we generate probability targets for this character.\n",
    "13. Then we fill in the data tensors for this character.\n",
    "14. Finally, we advance pointer to the next character. \"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\" Here is the explanation for the code above:\n",
    "1. We define a function called makeSyntheticDataFromRawSnippets that takes as input a set of character definitions, a set of snippets, the number of sentences to make, the number of steps in each sentence, and a list of words to use. It also takes some additional arguments that control the behavior of the function. \n",
    "2. The function makes a tensor that will hold the neural data, another tensor that will hold the character probabilities, and another tensor that will hold the character start signals. \n",
    "3. The function then loops through the number of sentences, and loops through each character in the sentence, picking a snippet for each character, and then adding the snippet to the neural data tensor and the character probability tensor. \n",
    "4. The function returns the neural data tensor, the character probability tensor, and the character start signal tensor. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SyntheticSentence:\n",
    "    def __init__(self):\n",
    "        self.vocabs = get_vocabs()\n",
    "\n",
    "    def word2idx(self, x: str) -> int:\n",
    "        return self.vocabs[x]\n",
    "    \n",
    "    def _extract_unique_characters(sentence: str) -> Set[str]:\n",
    "        # Use a set comprehension to create a set of unique characters\n",
    "        unique_chars = {char for char in sentence if char != ' '}\n",
    "        # Return the set of unique characters as a sorted list\n",
    "        return sorted(list(unique_chars))\n",
    "    \n",
    "    def _generate_character_probs(self, sentence: str) -> TensorType[\"seq_len\", \"n_vocabs\"]:\n",
    "        n_vocabs = len(self.vocabs)\n",
    "        \n",
    "        # TODO: don't hardcode\n",
    "        fixed_sentence = sentence.replace(\" \", \">\")\n",
    "        labels = [self.word2idx(x) for x in fixed_sentence]\n",
    "        labels = torch.tensor(labels)\n",
    "        \n",
    "        one_hot = F.one_hot(labels, n_vocabs)\n",
    "        return one_hot\n",
    "\n",
    "    def _generate_character_signal(self, sentence) -> TensorType[\"seq_len\"]:\n",
    "        seq_len = len(sentence)\n",
    "        x = torch.ones(seq_len)\n",
    "        x[-1] = 0\n",
    "        return x\n",
    "\n",
    "    def generate(self, sentence: str) -> Tuple[\n",
    "        TensorType[\"n_steps\", \"n_channels\"], # neural data\n",
    "        TensorType[\"seq_len\", \"n_vocabs\"], # character probabilities\n",
    "        TensorType[\"seq_len\"] # character signals\n",
    "    ]:\n",
    "        neural_data = torch.tensor([])\n",
    "        sentence = sentence.lower()  \n",
    "        for character in sentence:\n",
    "            character = \">\" if character == \" \" else character        \n",
    "            neural_template = load_snippet(character)[0][0]\n",
    "            neural_template = torch.tensor(neural_template)\n",
    "            neural_data = torch.cat((neural_data, neural_template), dim=0)\n",
    "            probs = self._generate_character_probs(sentence)\n",
    "            signals = self._generate_character_signal(sentence)\n",
    "        \n",
    "        return neural_data, probs, signals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
