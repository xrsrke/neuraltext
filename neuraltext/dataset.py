# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/05_dataset.ipynb.

# %% auto 0
__all__ = ['get_vocabs', 'TransformText', 'NeuralCharacterDataset', 'NeuralSentenceDataset']

# %% ../nbs/05_dataset.ipynb 4
from typing import List, Tuple, Set, Callable, Dict

import torch
from torch.utils.data import Dataset
from torchtyping import TensorType

# %% ../nbs/05_dataset.ipynb 7
def get_vocabs() -> Dict[str, int]:
    chars = ['a','b','c','d','e','f','g','h','i','j',
             'k','l','m','n','o','p','q','r','s','t',
             'u','v','w','x','y','z', '>',',',"'",'~','?'
    ]
    return {c: i for i, c in enumerate(chars)}

# %% ../nbs/05_dataset.ipynb 8
class TransformText:
    @staticmethod
    def encode(x: str) -> str:
        x = x.replace(',', '~')
        x = x.replace(' ', '>')
        return x

    @staticmethod
    def decode(x: str) -> str:
        x = x.replace('~', ',')
        x = x.replace('>', ' ')
        return x

# %% ../nbs/05_dataset.ipynb 9
class NeuralCharacterDataset(Dataset):
    def __init__(self, data: dict, tokenizer):
        xs = []
        ys = []
        vocabs = get_vocabs()
        
        for letter, items in data.items():
            for item in items:
                letter = self.preprocess(letter)
                xs.append(vocabs[letter])
                ys.append(torch.tensor(item).float())
        
        self.xs: List[str] = xs
        self.ys = ys
        self.vocabs = vocabs
    
    def preprocess(self, x: str) -> str:
        if x == "greaterThan":
            x = ">"
        elif x == "comma":
            x = ","
        elif x == "apostrophe":
            x = "'"
        elif x == "tilde":
            x = "~"
        elif x == "questionMark":
            x = "?"
        return x
    
    def __len__(self) -> int:
        return len(self.ys)

    def __getitem__(self, index) -> Tuple[int, TensorType['n_step', 'n_channel']]:
        return self.xs[index], self.ys[index]

# %% ../nbs/05_dataset.ipynb 10
class NeuralSentenceDataset(Dataset):
    """Dataset for neural sentences."""
    def __init__(self, data, tokenizer: Callable):
        xs = []
        ys = []
        
        for sentence, neural_data in data:
            tokenized_sentence = tokenizer(sentence, return_tensors="pt")["input_ids"][0]
            xs.append(tokenized_sentence)
            ys.append(torch.tensor(neural_data).float())
        
        self.xs = xs
        self.ys = ys
    
    def __getitem__(
        self, index: int
    ) -> Tuple[TensorType["seq_len"], TensorType["n_steps", "n_channels"]]:
        return self.xs[index], self.ys[index]
