# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/12_model.ipynb.

# %% auto 0
__all__ = ['RNN']

# %% ../nbs/12_model.ipynb 4
from typing import Optional, Tuple, Dict

from torch import nn
import torch.nn.functional as F

from torchtyping import TensorType
from einops import rearrange

from .dataset import get_vocabs

# %% ../nbs/12_model.ipynb 7
class RNN(nn.Module):
    """The Decoder Neural Signal."""
    def __init__(
        self,
        input_size: int = 192, # The firing rate
        hidden_size: int = 512,
        output_size: int = 31, # The number of vocabs
    ):
        super().__init__()
        self.gru1 = nn.GRU(input_size, hidden_size)
        self.gru2 = nn.GRU(hidden_size, output_size)
        self.z_layer = nn.Linear(hidden_size, 1)
    
    def _get_prob_next_character(
        self, x: TensorType["batch_size", "hidden_size"]
    ) -> TensorType[1]: # The probability that there will be a next character
        return F.sigmoid(self.z_layer(x))

    def forward(
        self,
        x: TensorType["batch_size", "n_vocabs"],
        hidden: Optional[TensorType["batch_size", "hidden_size"]] = None
    ) -> Tuple[
        TensorType["batch_size", "n_vocabs"], # the probabilty for each character
        TensorType["batch_size", "hidden_size"],
        TensorType[1]
    ]:
        """The forward pass."""
        gru1_out, hidden = self.gru1(x, hidden)
        probs = F.softmax(self.gru2(gru1_out)[0], dim=-1)
        z_t = self._get_prob_next_character(hidden)
        z_t = rearrange(z_t, '1 b -> b')
        return probs, hidden, z_t
